# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rq5BD2pv0fyM-UNwsPCPXib9y5b7AAla
"""

# =======================================================
# DETEKSI UJARAN KEBENCIAN MENGGUNAKAN LSTM (2 KELAS)
# Disesuaikan untuk dataset biner (0 dan 1)
# =======================================================

# Langkah 1: Import Library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.model_selection import train_test_split

import nltk
import string
import warnings
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud

import tensorflow as tf
from tensorflow import keras
from keras import layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import confusion_matrix, classification_report

# Unduh resource NLTK (Anti-Error)
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

try:
    nltk.data.find('corpora/wordnet')
    nltk.data.find('corpora/omw-1.4')
except LookupError:
    nltk.download('wordnet')
    nltk.download('omw-1.4')
warnings.filterwarnings('ignore')


# Inisialisasi Lemmatizer dan Stopwords di awal
lemmatizer = WordNetLemmatizer()
# Menggunakan set untuk pencarian yang lebih cepat
stop_words = set(stopwords.words('english'))

# Fungsi Pembersihan Teks
def remove_punctuations(text):
    # Hapus URL
    text = re.sub(r'http\S+|www.\S+', '', text)
    # Hapus mention (@user)
    text = re.sub(r'@\w+', '', text)
    # Hapus angka
    text = re.sub(r'\d+', '', text)

    temp = str.maketrans('', '', string.punctuation)
    return text.translate(temp)

def preprocess_text(text):
    text = str(text).lower()
    text = remove_punctuations(text)
    # Lemmatisasi dan Stopword Removal
    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words and len(word) > 1]
    return " ".join(words)


# ============================================
# Langkah 2: Memuat Dataset & Pra-pemrosesan Awal
# ============================================
df = pd.read_csv('train.csv')

# ASUMSI: Dataset memiliki kolom 'label' (0 atau 1) dan 'tweet'
# Mengubah nama kolom 'label' menjadi 'class' agar konsisten.
df.rename(columns={'tweet': 'tweet', 'label': 'class'}, inplace=True)

# Memastikan kolom 'class' bertipe integer (0 atau 1)
df['class'] = df['class'].astype(int)
df.dropna(subset=['tweet', 'class'], inplace=True)

# Terapkan Pre-processing PADA DATA ASLI sebelum split
df['tweet_clean'] = df['tweet'].apply(preprocess_text)

print(f"Shape Dataset Awal: {df.shape}")
# MEMULIHKAN df.info()
print("\n--- Ringkasan Dataset ---")
df.info()
print("--------------------------")

# Plotting Distribusi Kelas Asli (Binary)
plt.figure(figsize=(6, 6))
# Label disesuaikan untuk 2 kelas
class_counts = df['class'].value_counts()
labels = [f'Non-Hate/Offensive (0) - {class_counts[0]}' if 0 in class_counts else 'Non-Hate/Offensive (0) (Missing)',
          f'Hate/Offensive (1) - {class_counts[1]}' if 1 in class_counts else 'Hate/Offensive (1) (Missing)']
plt.pie(class_counts.values,
        labels=labels,
        autopct='%1.1f%%',
        startangle=90)
plt.title("Distribusi Kelas Asli (2 Kelas)")
plt.savefig('distribusi_kelas_asli_binary.png')
plt.show()

# ============================================
# Langkah 3: Split Data DAHULU
# ============================================
features = df['tweet_clean']
target = df['class']

X_train_unbalanced, X_val, Y_train_unbalanced, Y_val = train_test_split(
    features, target, test_size=0.2, random_state=42, stratify=target
)

df_train = pd.DataFrame({'tweet_clean': X_train_unbalanced, 'class': Y_train_unbalanced})


# ============================================
# Langkah 4: Menyeimbangkan Data HANYA pada Training Set (Binary Oversampling)
# ============================================
class_0 = df_train[df_train['class'] == 0]
class_1 = df_train[df_train['class'] == 1]

# Menentukan ukuran target (diambil dari kelas mayoritas, biasanya Kelas 0)
target_n = len(class_0)

# Oversample Kelas 1 (minoritas) untuk menyamai Kelas 0 (mayoritas)
class_1_oversampled = class_1.sample(n=target_n, replace=True, random_state=42)

# Menggabungkan dan mengacak kembali
balanced_train_df = pd.concat([class_0, class_1_oversampled], axis=0)
balanced_train_df = balanced_train_df.sample(frac=1, random_state=42).reset_index(drop=True)

X_train = balanced_train_df['tweet_clean']
Y_train = balanced_train_df['class']

print(f"\nShape Training Set Seimbang: {balanced_train_df.shape}")

# Plotting Distribusi Kelas Seimbang (Training Set)
plt.figure(figsize=(6, 6))
class_counts_balanced = Y_train.value_counts()
labels_balanced = [f'Non-Hate/Offensive (0) - {class_counts_balanced[0]}',
                   f'Hate/Offensive (1) - {class_counts_balanced[1]}']
plt.pie(class_counts_balanced.values,
        labels=labels_balanced,
        autopct='%1.1f%%',
        startangle=90)
plt.title("Distribusi Kelas Training (Setelah Penyeimbangan)")
plt.savefig('distribusi_kelas_seimbang_binary.png')
plt.show()


# WordCloud (Menggunakan data training yang sudah seimbang)
def plot_word_cloud(data, typ, filename):
    corpus = " ".join(data['tweet_clean'])
    wc = WordCloud(max_words=100, width=800, height=400, collocations=False).generate(corpus)
    plt.figure(figsize=(10, 5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"Word Cloud for {typ} Class (Balanced Training)", fontsize=15)
    plt.savefig(filename)
    plt.show()

# Plot Word Cloud untuk setiap kelas
plot_word_cloud(balanced_train_df[balanced_train_df['class'] == 0], typ="Non-Hate/Offensive (0)", filename='wc_non_hate_binary.png')
plot_word_cloud(balanced_train_df[balanced_train_df['class'] == 1], typ="Hate/Offensive (1)", filename='wc_hate_binary.png')


# ============================================
# Langkah 5: Tokenisasi dan Padding
# ============================================
# One-Hot Encoding otomatis akan menghasilkan 2 kolom karena input Y_train hanya memiliki nilai 0 dan 1
Y_train_ohe = pd.get_dummies(Y_train)
Y_val_ohe = pd.get_dummies(Y_val)

max_words = 10000
max_len = 100

tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)

X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')
X_val_padded = pad_sequences(X_val_seq, maxlen=max_len, padding='post', truncating='post')

# ============================================
# Langkah 6: Membangun Model LSTM
# ============================================
# Jumlah unit output layer disesuaikan menjadi 2 untuk klasifikasi biner
NUM_CLASSES = 2
model = keras.models.Sequential([
    layers.Embedding(input_dim=max_words, output_dim=32, input_length=max_len),
    layers.Bidirectional(layers.LSTM(16)),
    layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l1(0.001)),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    layers.Dense(NUM_CLASSES, activation='softmax') # FINAL LAYER: 2 UNIT
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# MEMULIHKAN model.summary() AGAR TAMPIL SEBELUM TRAINING
print("\n--- Ringkasan Model (SEBELUM TRAINING) ---")
model.summary()
print("------------------------------------------")

# ============================================
# Langkah 7: Melatih Model
# ============================================
es = EarlyStopping(patience=3, monitor='val_accuracy', restore_best_weights=True)
lr = ReduceLROnPlateau(patience=2, monitor='val_loss', factor=0.5, verbose=1)


history = model.fit(
    X_train_padded, Y_train_ohe,
    validation_data=(X_val_padded, Y_val_ohe),
    epochs=50,
    batch_size=32,
    callbacks=[es, lr],
    verbose=1
)

# KOREKSI: Panggil model.summary() setelah model dilatih (Total params terhitung)
print("\n--- Ringkasan Model (SETELAH TRAINING) ---")
model.summary()
print("------------------------------------------")

# ============================================
# Langkah 8: Evaluasi Model
# ============================================
test_loss, test_acc = model.evaluate(X_val_padded, Y_val_ohe, verbose=0)
print(f"\nValidation Accuracy: {test_acc:.4f}")

# Tambahan: Laporan Klasifikasi dan Confusion Matrix
from sklearn.metrics import classification_report, confusion_matrix
Y_pred_probs = model.predict(X_val_padded)
Y_pred = np.argmax(Y_pred_probs, axis=1)
Y_true = Y_val.values

print("\n--- Classification Report ---")
print(classification_report(Y_true, Y_pred, target_names=['Non-Hate/Offensive (0)', 'Hate/Offensive (1)']))

print("\n--- Confusion Matrix ---")
cm = confusion_matrix(Y_true, Y_pred)
print(cm)

plt.figure(figsize=(8, 6))
sb.heatmap(cm, annot=True, fmt='d', cmap='Blues',
           xticklabels=['Non-Hate/Offensive (0)', 'Hate/Offensive (1)'],
           yticklabels=['Non-Hate/Offensive (0)', 'Hate/Offensive (1)'])
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.savefig('confusion_matrix_binary.png')
plt.show()

# Plotting Loss dan Accuracy
history_df = pd.DataFrame(history.history)
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
history_df[['loss', 'val_loss']].plot(ax=plt.gca(), title="Loss")
plt.subplot(1, 2, 2)
history_df[['accuracy', 'val_accuracy']].plot(ax=plt.gca(), title="Accuracy")
plt.tight_layout()
plt.savefig('loss_accuracy_plot_binary_FIXED.png')
plt.show()

# Simpan hasil
results_df = pd.DataFrame({
    'Dataset': ['train.csv (Binary)'],
    'Validation Accuracy': [test_acc],
    'Validation Loss': [test_loss]
})
results_df.to_csv('lstm_results_binary_fixed.csv', index=False)


print("\nSemua file plot dan hasil telah disimpan ke direktori kerja dan ditampilkan.")